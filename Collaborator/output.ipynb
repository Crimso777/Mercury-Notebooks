{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f516501",
   "metadata": {},
   "source": [
    "#HELLO WORLD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1v9c5_5mMph"
   },
   "source": [
    "**Instructions:**\n",
    "\n",
    "\n",
    "1.   Submit both .ipynb and PDF files, No pdf or no .ipynb will get '0'.\n",
    "2.   Do not ZIP the files and submit, Zip submission will get '0'.\n",
    "3.   Provide Observations for all questions.\n",
    "4.   **Similarity score must be less than 15%**\n",
    "5.   Don't use same datasets used in tutorials for tasks.\n",
    "6.   click here for datasets used in tutorials [click here](https://drive.google.com/file/d/1ZT3hnZHeoCht-URF3zPND4JMMz9CGXWb/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkJLgpmSmS0G"
   },
   "source": [
    "**RUBRIC:**\n",
    "\n",
    "\n",
    "**TASK 1 :** **10pts full pts | 0pts No pts**\n",
    "\n",
    "**TASK 2 :** **10pts full pts | 0pts No Ans**\n",
    "\n",
    "**Task 3 :** **25pts Full Marks | 0pts No Answer**\n",
    "\n",
    "**TASK 4 :**   **30pts Full marks | 0pts No Answer**\n",
    "\n",
    "**TASK 5 :** **10pts full marks | 0pts No Answer**\n",
    "\n",
    "**TASK 6 :** **10pts full marks | 0pts No Ans**\n",
    "\n",
    "**TASK 7 :** **5pts full marks | 0pts No Ans**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBgH0Dc08SeX"
   },
   "source": [
    "#**Spelling Correction in Natural Language Processing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKEF5g3P-r9i"
   },
   "source": [
    "Please follow the tutorials below and complete the tasks that are available on the webpages provided. The tutorials will have code and might not have dataset file. You can create dataset files as per the tutorial requirements. The aim of the ICE is to make the tutorials in executed format. You can also use the github repositories of the authors if they are avilable. It is recommended to run the tutorials and then test it with your own datasets (Custom made). You can use any source on the internet to complete the tasks. For task 4 your have to provide mini-examples to differentiate between non-word and real world spelling corrections. After the code for task 4 please provide a brief explanation for what is the difference and what is your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAYfpbp-9lMV"
   },
   "source": [
    "# Tutorial 1:\n",
    "### Use the following tutorial to implement spelling checking using Textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3Le0vQsJ6Qt",
    "outputId": "ac4350f7-4c22-4cac-c3a8-61f16d3a4304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwqRXzlsLwV8"
   },
   "source": [
    "**The correct() Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_YYKrXd-PK0",
    "outputId": "28aaf800-2c4c-4af3-a509-c6601c11aa40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of we admit the the geological recording is imperfect in an extreme degree, than such facts as the record gives, support the theory of descent with modification. New species hive come on the stage slowly and at successive intervals; and the amount of change, after equal intervals of time, is widely different in different groups.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "with open(\"text.txt\", \"r\") as f:        # Opening the test file with the intention to read\n",
    "    text = f.read()                     # Reading the file\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    print(textCorrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2azk4t2L4u8"
   },
   "source": [
    "###**How Correct is TextBlob's Spelling Correction?**\n",
    "As we can see, the text still has some spelling errors. Words like \"abl\" were supposed to be \"able\", not \"all\". Though, even with these, it's still better than the original.\n",
    "\n",
    "**Now comes the question, how much better is it?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjciYbZnMcFG"
   },
   "source": [
    "###The following code snippet is a simple script that test how good is TextBlob in correcting errors, based on this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "UnM_RPoU-PPS"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# A function that compares two texts and returns \n",
    "# the number of matches and differences\n",
    "def compare(text1, text2):  \n",
    "    l1 = text1.split()\n",
    "    l2 = text2.split()\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    for i in range(0, len(l1)):\n",
    "        if l1[i] != l2[i]:\n",
    "            bad += 1\n",
    "        else:\n",
    "            good += 1\n",
    "    return (good, bad)\n",
    "\n",
    "# Helper function to calculate the percentage of misspelled words\n",
    "def percentageOfBad(x):\n",
    "    return (x[1] / (x[0] + x[1])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UzBEkms-PTH",
    "outputId": "44c72794-e847-4090-d787-26c6fc2dbf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistakes compared to original  (43, 12)\n",
      "Original compared to corrected  (50, 5)\n",
      "Mistakes compared to corrected  (44, 11) \n",
      "\n",
      "Percentage of mistakes in the test:  21.818181818181817 %\n",
      "Percentage of mistakes in the corrected:  9.090909090909092 %\n",
      "Percentage of fixed mistakes:  20.0 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\", \"r\") as f1: # test.txt contains the same typo-filled text from the last example \n",
    "    t1 = f1.read()\n",
    "\n",
    "with open(\"original.txt\", \"r\") as f2: # original.txt contains the text from the actual book \n",
    "    t2 = f2.read()\n",
    "\n",
    "t3 = TextBlob(t1).correct()\n",
    "\n",
    "mistakesCompOriginal = compare(t1, t2)\n",
    "originalCompCorrected = compare(t2, t3)\n",
    "mistakesCompCorrected = compare(t1, t3)\n",
    "\n",
    "print(\"Mistakes compared to original \", mistakesCompOriginal)\n",
    "print(\"Original compared to corrected \", originalCompCorrected)\n",
    "print(\"Mistakes compared to corrected \", mistakesCompCorrected, \"\\n\")\n",
    "\n",
    "print(\"Percentage of mistakes in the test: \", percentageOfBad(mistakesCompOriginal), \"%\")\n",
    "print(\"Percentage of mistakes in the corrected: \", percentageOfBad(originalCompCorrected), \"%\")\n",
    "print(\"Percentage of fixed mistakes: \", percentageOfBad(mistakesCompCorrected), \"%\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kxb4RXqOoz8"
   },
   "source": [
    "###As we can see, the correct method managed to get our spelling mistake percentage from 20% to 9%, which is pretty decent, however there's a bit of a catch. It corrected 20% of the words, so why is there still a 0.59% mistake rate?\n",
    "\n",
    "The answer is overcorrection. Sometimes, it can change a word that is spelled correctly, like the first word in our example text where \"As\" was corrected to \"Is\". Other times, it just doesn't have enough information about the word and the context to tell which word the user was intending to type, so it guesses that it should replace \"whl\" with \"while\" instead of \"whole\".\n",
    "\n",
    "There is no perfect spelling corrector because so much of spoken language is contextual, so keep that in mind. In most use cases, there are way fewer mistakes than in our example, so TextBlob should be able to work well enough for the average user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppuKbgkMvW9O"
   },
   "source": [
    "# Task 1:\n",
    "### Use the above tutorial to implement spelling checking using Textblob on your own dataset.\n",
    "###write your own observation on the functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "P6mc2_HGvf0Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Yj93ogeUvhEn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "FuiQRABdvgeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "m5rtAlbAvgM-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MXH3VV28eOj"
   },
   "source": [
    "# Tutorial 2: \n",
    "###Training TextBlob with Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "BwTzXV6quZPd"
   },
   "outputs": [],
   "source": [
    "from textblob.en import Spelling        \n",
    "import re\n",
    "\n",
    "textToLower = \"\"\n",
    "\n",
    "with open(\"originOfSpecies.txt\",\"r\") as f1:           # Open our source file\n",
    "\ttext = f1.read()                                  # Read the file                 \n",
    "\ttextToLower = text.lower()                        # Lower all the capital letters\n",
    "\n",
    "words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
    "oneString = \" \".join(words)                           # Join them into one string\n",
    "\n",
    "pathToFile = \"train.txt\"                              # The path we want to store our stats file at\n",
    "spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
    "spelling.train(oneString, pathToFile)                 # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgoRzgHcunf-",
    "outputId": "5292fc1e-7a3d-47a4-d7b5-b83c01ec5a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of we admit the the geological record is imperfect in an extreme degree than such facts as the record gives support the theory of descent with modification New species hive come on the stage slowly and at successive intervals and the amount of change after equal intervals of time is widely different in different groups\n"
     ]
    }
   ],
   "source": [
    "from textblob.en import Spelling        \n",
    "from textblob import TextBlob\n",
    "\n",
    "pathToFile = \"train.txt\" \n",
    "spelling = Spelling(path = pathToFile)\n",
    "text = \" \"\n",
    "\n",
    "with open(\"test.txt\", \"r\") as f: \n",
    "\ttext = f.read()\n",
    "\n",
    "words = text.split()\n",
    "corrected = \" \"\n",
    "for i in words :\n",
    "    corrected = corrected +\" \"+ spelling.suggest(i)[0][0] # Spell checking word by word\n",
    "\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZAxshfyu2qE"
   },
   "source": [
    "###This fixes around 2 out of 3 of misspelled words, which is pretty good, considering the run without much context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S83mtDImu6Vg"
   },
   "source": [
    "#**Conclusion**\n",
    "Here used TextBlob to implement a basic spelling corrector, both with the stock prediction model a custom one.\n",
    "\n",
    "Correcting man-made spelling errors has become a common task for software developers. Even though it has become easier and more efficient via data mining, many spelling mistakes need context to be corrected.\n",
    "\n",
    "In conclusion, proofreaders are probably not going to get automated out of work any time soon, though, some basic correction can be automated to save time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGHiI5KMuSB4"
   },
   "source": [
    "# Task 2: \n",
    "### Train your Textblob model on your own dataset\n",
    "### Instructions are provided in the above *tutorial*\n",
    "\n",
    "###write your own observation on the functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "XVQ7Td-c-P-W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "48P-U2uQ-QHI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Z1Z8tLdu-QMB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "I3Vjj9Mt-QP0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orglcNDl8x6d"
   },
   "source": [
    "# Tutorial 3: \n",
    "### Implementation Petr Norvig algorithm for spelling corrections. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11wtQcRJ-Q9u",
    "outputId": "e76d7b73-cd69-4558-ea08-b2ea014f6ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_tests pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75% of 270 correct (6% unknown) at 41 words per second \n",
      "68% of 400 correct (11% unknown) at 46 words per second \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('/content/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "################ Test Code \n",
    "\n",
    "def unit_tests():\n",
    "    assert correction('speling') == 'spelling'              # insert\n",
    "    assert correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
    "    assert Counter(words('This is a test. 123; A TEST this is.')) == (\n",
    "           Counter({'123': 1, 'a': 2, 'is': 2, 'test': 2, 'this': 2}))\n",
    "    assert len(WORDS) == 32198\n",
    "    assert sum(WORDS.values()) == 1115585\n",
    "    assert WORDS.most_common(10) == [\n",
    " ('the', 79809),\n",
    " ('of', 40024),\n",
    " ('and', 38312),\n",
    " ('to', 28765),\n",
    " ('in', 22023),\n",
    " ('a', 21124),\n",
    " ('that', 12512),\n",
    " ('he', 12401),\n",
    " ('was', 11410),\n",
    " ('it', 10681)]\n",
    "    assert WORDS['the'] == 79809\n",
    "    assert P('quintessential') == 0\n",
    "    assert 0.07 < P('the') < 0.08\n",
    "    return 'unit_tests pass'\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
    "    import time\n",
    "    start = time.clock()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "    for right, wrong in tests:\n",
    "        w = correction(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in WORDS)\n",
    "            if verbose:\n",
    "                print('correction({}) => {} ({}); expected {} ({})'\n",
    "                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "    dt = time.clock() - start\n",
    "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "          .format(good / n, n, unknown / n, n / dt))\n",
    "    \n",
    "def Testset(lines):\n",
    "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "    return [(right, wrong)\n",
    "            for (right, wrongs) in (line.split(':') for line in lines)\n",
    "            for wrong in wrongs.split()]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(unit_tests())\n",
    "    spelltest(Testset(open('/content/spell-testset1.txt')))\n",
    "    spelltest(Testset(open('/content/spell-testset2.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ps3VqqapZjD1",
    "outputId": "6e0cb5a4-fccb-44f7-ae23-685b3745bf37"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'just'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('juse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnF-b6xWZMCO"
   },
   "source": [
    "###So on the development set we get 75% correct (processing words at a rate of 41 words/second), and on the final test set we get 68% correct (at 46 words/second). In conclusion, I met my goals for brevity, development time, and runtime speed, but not for accuracy. Perhaps my test set was extra tough, or perhaps my simple model is just not good enough to get to 80% or 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNwUohyCOVfK"
   },
   "source": [
    "# Task 3: \n",
    "### Perform Petr Norvig algorithm for spelling corrections  using other dataset.\n",
    "###Also provide your observation for this algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "2JoROvKSPJY5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "YiiOz9EAPJK0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "GpIKev07PI-j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "yPZP8pdL-RET"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "9SMcdxJ7-RHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYpAfTKD9Kg1"
   },
   "source": [
    "# Task 4: (40%)\n",
    "### Implement spelling correction using Noisy Channel for non-word and real word. You can follow any code\n",
    "\n",
    "###provide your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "R84pssh4-RxE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "tC8lpGZt-R0V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "e_NBwBgW-R5V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "FtMhId4I-R85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn05MM8R5KJX"
   },
   "source": [
    "#Task 5:\n",
    "\n",
    "###Implement spelling correction using edit distance model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "IAbTsqn55ctf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "wFFc07Ik5dUx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "QZIc8nuo5dEI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRkWVzkh5jDP"
   },
   "source": [
    "#Task:6\n",
    "\n",
    "###Implement spelling correction using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "o-Tk--QGdHz3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "L_D4FA82dHW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Hx060VbDEtl2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJFh-SHYi79i"
   },
   "source": [
    "#Task 7:\n",
    "###Explain your observation for spelling correction using edit distance and spacy. which one do you think is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTdC6QYgkay0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4ec9qWGkZph"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEevqSbWszCl"
   },
   "source": [
    "###REFERENCES\n",
    "1) \n",
    "https://stackabuse.com/spelling-correction-in-python-with-textblob/\n",
    "\n",
    "2) \n",
    "https://medium.com/mlearning-ai/build-spell-checking-models-for-any-language-in-python-aa4489df0a5f"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
